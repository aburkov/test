\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[mar()]{marker}
Marker.
\newblock URL \url{https://github.com/datalab-to/marker}.

\bibitem[mat()]{mathpix}
Mathpix.
\newblock URL \url{https://mathpix.com/}.

\bibitem[ocr(2025)]{ocrflux}
Ocrflux, 2025.
\newblock URL \url{https://github.com/chatdoc-com/OCRFlux}.

\bibitem[AI(2025)]{google_gemini_web}
G.~AI.
\newblock Gemini 2.5-pro, 2025.
\newblock URL \url{https://gemini.google.com/}.

\bibitem[Bai et~al.(2025)Bai, Chen, Liu, Wang, Ge, Song, Dang, Wang, Wang, Tang, Zhong, Zhu, Yang, Li, Wan, Wang, Ding, Fu, Xu, Ye, Zhang, Xie, Cheng, Zhang, Yang, Xu, and Lin]{Qwen2.5-VL}
S.~Bai, K.~Chen, X.~Liu, J.~Wang, W.~Ge, S.~Song, K.~Dang, P.~Wang, S.~Wang, J.~Tang, H.~Zhong, Y.~Zhu, M.~Yang, Z.~Li, J.~Wan, P.~Wang, W.~Ding, Z.~Fu, Y.~Xu, J.~Ye, X.~Zhang, T.~Xie, Z.~Cheng, H.~Zhang, Z.~Yang, H.~Xu, and J.~Lin.
\newblock Qwen2.5-vl technical report.
\newblock \emph{arXiv preprint arXiv:2502.13923}, 2025.

\bibitem[Blecher et~al.(2023)Blecher, Cucurull, Scialom, and Stojnic]{blecher2023nougat}
L.~Blecher, G.~Cucurull, T.~Scialom, and R.~Stojnic.
\newblock Nougat: Neural optical understanding for academic documents.
\newblock \emph{arXiv preprint arXiv:2308.13418}, 2023.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Kong, Wei, Liu, Ge, Zhao, Sun, Han, and Zhang]{chen2024onechart}
J.~Chen, L.~Kong, H.~Wei, C.~Liu, Z.~Ge, L.~Zhao, J.~Sun, C.~Han, and X.~Zhang.
\newblock Onechart: Purify the chart structural extraction via one auxiliary token.
\newblock In \emph{Proceedings of the 32nd ACM International Conference on Multimedia}, pages 147--155, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Wang, Tian, Ye, Gao, Cui, Tong, Hu, Luo, Ma, et~al.]{chen2024internvl2}
Z.~Chen, W.~Wang, H.~Tian, S.~Ye, Z.~Gao, E.~Cui, W.~Tong, K.~Hu, J.~Luo, Z.~Ma, et~al.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock \emph{arXiv preprint arXiv:2404.16821}, 2024{\natexlab{b}}.

\bibitem[Cui et~al.(2025)Cui, Sun, Lin, Gao, Zhang, Liu, Wang, Zhang, Zhou, Liu, et~al.]{cui2025paddleocr}
C.~Cui, T.~Sun, M.~Lin, T.~Gao, Y.~Zhang, J.~Liu, X.~Wang, Z.~Zhang, C.~Zhou, H.~Liu, et~al.
\newblock Paddleocr 3.0 technical report.
\newblock \emph{arXiv preprint arXiv:2507.05595}, 2025.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek, Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023patch}
M.~Dehghani, J.~Djolonga, B.~Mustafa, P.~Padlewski, J.~Heek, J.~Gilmer, A.~Steiner, M.~Caron, R.~Geirhos, I.~Alabdulmohsin, et~al.
\newblock Patch n' pack: Navit, a vision transformer for any aspect ratio and resolution.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 3632--3656, 2023.

\bibitem[Feng et~al.(2025)Feng, Wei, Fei, Shi, Han, Liao, Lu, Wu, Liu, Lin, et~al.]{feng2025dolphin}
H.~Feng, S.~Wei, X.~Fei, W.~Shi, Y.~Han, L.~Liao, J.~Lu, B.~Wu, Q.~Liu, C.~Lin, et~al.
\newblock Dolphin: Document image parsing via heterogeneous anchor prompting.
\newblock \emph{arXiv preprint arXiv:2505.14059}, 2025.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and Parikh]{goyal2017making}
Y.~Goyal, T.~Khot, D.~Summers-Stay, D.~Batra, and D.~Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6904--6913, 2017.

\bibitem[Gu et~al.(2022)Gu, Meng, Lu, Hou, Minzhe, Liang, Yao, Huang, Zhang, Jiang, et~al.]{gu2022wukong}
J.~Gu, X.~Meng, G.~Lu, L.~Hou, N.~Minzhe, X.~Liang, L.~Yao, R.~Huang, W.~Zhang, X.~Jiang, et~al.
\newblock Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26418--26431, 2022.

\bibitem[High-flyer(2023)]{highflyer2023haillm}
High-flyer.
\newblock {HAI-LLM}: Efficient and lightweight training tool for large models, 2023.
\newblock URL \url{https://www.high-flyer.cn/en/blog/hai-llm}.

\bibitem[Iyer et~al.(2022)Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster, Wang, Liu, Koura, et~al.]{OPT-IML}
S.~Iyer, X.~V. Lin, R.~Pasunuru, T.~Mihaylov, D.~Simig, P.~Yu, K.~Shuster, T.~Wang, Q.~Liu, P.~S. Koura, et~al.
\newblock Opt-iml: Scaling language model instruction meta learning through the lens of generalization.
\newblock \emph{arXiv preprint arXiv:2212.12017}, 2022.

\bibitem[Kazemzadeh et~al.(2014)Kazemzadeh, Ordonez, Matten, and Berg]{kazemzadeh2014referitgame}
S.~Kazemzadeh, V.~Ordonez, M.~Matten, and T.~Berg.
\newblock Referitgame: Referring to objects in photographs of natural scenes.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)}, pages 787--798, 2014.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson, Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
A.~Kirillov, E.~Mintun, N.~Ravi, H.~Mao, C.~Rolland, L.~Gustafson, T.~Xiao, S.~Whitehead, A.~C. Berg, W.-Y. Lo, et~al.
\newblock Segment anything.
\newblock \emph{arXiv preprint arXiv:2304.02643}, 2023.

\bibitem[Li et~al.(2025)Li, Liu, Liu, Ma, Zhang, Zhang, Guo, Zhang, Wang, and Bai]{li2025monkeyocr}
Z.~Li, Y.~Liu, Q.~Liu, Z.~Ma, Z.~Zhang, S.~Zhang, Z.~Guo, J.~Zhang, X.~Wang, and X.~Bai.
\newblock Monkeyocr: Document parsing with a structure-recognition-relation triplet paradigm.
\newblock \emph{arXiv preprint arXiv:2506.05218}, 2025.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Feng, Wang, Wang, Liu, Zhao, Dengr, Ruan, Dai, Guo, et~al.]{liu2024deepseekv2}
A.~Liu, B.~Feng, B.~Wang, B.~Wang, B.~Liu, C.~Zhao, C.~Dengr, C.~Ruan, D.~Dai, D.~Guo, et~al.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.
\newblock \emph{arXiv preprint arXiv:2405.04434}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Feng, Xue, Wang, Wu, Lu, Zhao, Deng, Zhang, Ruan, et~al.]{liu2024deepseekv3}
A.~Liu, B.~Feng, B.~Xue, B.~Wang, B.~Wu, C.~Lu, C.~Zhao, C.~Deng, C.~Zhang, C.~Ruan, et~al.
\newblock Deepseek-v3 technical report.
\newblock \emph{arXiv preprint arXiv:2412.19437}, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Wei, Chen, Kong, Ge, Zhu, Zhao, Sun, Han, and Zhang]{liu2024focus_fox}
C.~Liu, H.~Wei, J.~Chen, L.~Kong, Z.~Ge, Z.~Zhu, L.~Zhao, J.~Sun, C.~Han, and X.~Zhang.
\newblock Focus anywhere for fine-grained multi-page document understanding.
\newblock \emph{arXiv preprint arXiv:2405.14295}, 2024{\natexlab{c}}.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Loshchilov and Hutter(2019)]{AdamW}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Masry et~al.(2022)Masry, Long, Tan, Joty, and Hoque]{masry2022chartqa}
A.~Masry, D.~X. Long, J.~Q. Tan, S.~Joty, and E.~Hoque.
\newblock Chartqa: A benchmark for question answering about charts with visual and logical reasoning.
\newblock \emph{arXiv preprint arXiv:2203.10244}, 2022.

\bibitem[Nassar et~al.(2025)Nassar, Marafioti, Omenetti, Lysak, Livathinos, Auer, Morin, de~Lima, Kim, Gurbuz, et~al.]{nassar2025smoldocling}
A.~Nassar, A.~Marafioti, M.~Omenetti, M.~Lysak, N.~Livathinos, C.~Auer, L.~Morin, R.~T. de~Lima, Y.~Kim, A.~S. Gurbuz, et~al.
\newblock Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion.
\newblock \emph{arXiv preprint arXiv:2503.11576}, 2025.

\bibitem[OpenAI(2023)]{GPT4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2025)Ouyang, Qu, Zhou, Zhu, Zhang, Lin, Wang, Zhao, Jiang, Zhao, et~al.]{ouyang2025omnidocbench}
L.~Ouyang, Y.~Qu, H.~Zhou, J.~Zhu, R.~Zhang, Q.~Lin, B.~Wang, Z.~Zhao, M.~Jiang, X.~Zhao, et~al.
\newblock Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations.
\newblock In \emph{Proceedings of the Computer Vision and Pattern Recognition Conference}, pages 24838--24848, 2025.

\bibitem[Poznanski et~al.(2025)Poznanski, Rangapur, Borchardt, Dunkelberger, Huff, Lin, Wilhelm, Lo, and Soldaini]{poznanski2025olmocr}
J.~Poznanski, A.~Rangapur, J.~Borchardt, J.~Dunkelberger, R.~Huff, D.~Lin, C.~Wilhelm, K.~Lo, and L.~Soldaini.
\newblock olmocr: Unlocking trillions of tokens in pdfs with vision language models.
\newblock \emph{arXiv preprint arXiv:2502.18443}, 2025.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Rednote(2025)]{dots}
Rednote.
\newblock dots.ocr, 2025.
\newblock URL \url{https://github.com/rednote-hilab/dots.ocr}.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis, Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
C.~Schuhmann, R.~Vencu, R.~Beaumont, R.~Kaczmarczyk, C.~Mullis, A.~Katta, T.~Coombes, J.~Jitsev, and A.~Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Singh et~al.(2019)Singh, Natarajan, Shah, Jiang, Chen, Batra, Parikh, and Rohrbach]{TextVQA}
A.~Singh, V.~Natarajan, M.~Shah, Y.~Jiang, X.~Chen, D.~Batra, D.~Parikh, and M.~Rohrbach.
\newblock Towards vqa models that can read.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem[Sun et~al.(2025)Sun, Cui, Du, and Liu]{sun2025pp}
T.~Sun, C.~Cui, Y.~Du, and Y.~Liu.
\newblock Pp-doclayout: A unified document layout detection model to accelerate large-scale data construction.
\newblock \emph{arXiv preprint arXiv:2503.17213}, 2025.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Xu, Zhao, Ouyang, Wu, Zhao, Xu, Liu, Qu, Shang, et~al.]{wang2024mineru}
B.~Wang, C.~Xu, X.~Zhao, L.~Ouyang, F.~Wu, Z.~Zhao, R.~Xu, K.~Liu, Y.~Qu, F.~Shang, et~al.
\newblock Mineru: An open-source solution for precise document content extraction.
\newblock \emph{arXiv preprint arXiv:2409.18839}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, et~al.]{wang2024qwen2}
P.~Wang, S.~Bai, S.~Tan, S.~Wang, Z.~Fan, J.~Bai, K.~Chen, X.~Liu, J.~Wang, W.~Ge, et~al.
\newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.
\newblock \emph{arXiv preprint arXiv:2409.12191}, 2024{\natexlab{b}}.

\bibitem[Wei et~al.(2024{\natexlab{a}})Wei, Kong, Chen, Zhao, Ge, Yang, Sun, Han, and Zhang]{wei2024vary}
H.~Wei, L.~Kong, J.~Chen, L.~Zhao, Z.~Ge, J.~Yang, J.~Sun, C.~Han, and X.~Zhang.
\newblock Vary: Scaling up the vision vocabulary for large vision-language model.
\newblock In \emph{European Conference on Computer Vision}, pages 408--424. Springer, 2024{\natexlab{a}}.

\bibitem[Wei et~al.(2024{\natexlab{b}})Wei, Kong, Chen, Zhao, Ge, Yu, Sun, Han, and Zhang]{wei2024small}
H.~Wei, L.~Kong, J.~Chen, L.~Zhao, Z.~Ge, E.~Yu, J.~Sun, C.~Han, and X.~Zhang.
\newblock Small language model meets with reinforced vision vocabulary.
\newblock \emph{arXiv preprint arXiv:2401.12503}, 2024{\natexlab{b}}.

\bibitem[Wei et~al.(2024{\natexlab{c}})Wei, Liu, Chen, Wang, Kong, Xu, Ge, Zhao, Sun, Peng, et~al.]{wei2024general}
H.~Wei, C.~Liu, J.~Chen, J.~Wang, L.~Kong, Y.~Xu, Z.~Ge, L.~Zhao, J.~Sun, Y.~Peng, et~al.
\newblock General ocr theory: Towards ocr-2.0 via a unified end-to-end model.
\newblock \emph{arXiv preprint arXiv:2409.01704}, 2024{\natexlab{c}}.

\bibitem[Wei et~al.(2024{\natexlab{d}})Wei, Yin, Li, Wang, Zhao, Sun, Ge, Zhang, and Jiang]{wei2024slow}
H.~Wei, Y.~Yin, Y.~Li, J.~Wang, L.~Zhao, J.~Sun, Z.~Ge, X.~Zhang, and D.~Jiang.
\newblock Slow perception: Let's perceive geometric figures step-by-step.
\newblock \emph{arXiv preprint arXiv:2412.20631}, 2024{\natexlab{d}}.

\bibitem[Wu et~al.(2024)Wu, Chen, Pan, Liu, Liu, Dai, Gao, Ma, Wu, Wang, et~al.]{wu2024deepseek}
Z.~Wu, X.~Chen, Z.~Pan, X.~Liu, W.~Liu, D.~Dai, H.~Gao, Y.~Ma, C.~Wu, B.~Wang, et~al.
\newblock Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding.
\newblock \emph{arXiv preprint arXiv:2412.10302}, 2024.

\bibitem[Yu et~al.(2023)Yu, Yang, Li, Wang, Lin, Liu, Wang, and Wang]{yu2023mm}
W.~Yu, Z.~Yang, L.~Li, J.~Wang, K.~Lin, Z.~Liu, X.~Wang, and L.~Wang.
\newblock Mm-vet: Evaluating large multimodal models for integrated capabilities.
\newblock \emph{arXiv preprint arXiv:2308.02490}, 2023.

\bibitem[Zhu et~al.(2025)Zhu, Wang, Chen, Liu, Ye, Gu, Tian, Duan, Su, Shao, et~al.]{zhu2025internvl3}
J.~Zhu, W.~Wang, Z.~Chen, Z.~Liu, S.~Ye, L.~Gu, H.~Tian, Y.~Duan, W.~Su, J.~Shao, et~al.
\newblock Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models.
\newblock \emph{arXiv preprint arXiv:2504.10479}, 2025.

\end{thebibliography}
